{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0yG24uWpHie"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate bitsandbytes pillow camelot-py[cv]\n",
        "!pip install langchain chromadb sentence-transformers  pillow gradio\n",
        "!pip install -qU langchain-text-splitters\n",
        "!pip install langchain-chroma\n",
        "!pip install langchain-huggingface\n",
        "!pip install PyMuPDF\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install  pandas faiss-cpu\n",
        "!apt-get install ghostscript python3-tk -y\n",
        "!pip install optimum"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import os\n",
        "\n",
        "PAGE_IMG_DIR = \"/content/page_images\"\n",
        "os.makedirs(PAGE_IMG_DIR, exist_ok=True)\n",
        "\n",
        "def render_pages(pdf_path, dpi=300):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    zoom = dpi / 72\n",
        "    mat = fitz.Matrix(zoom, zoom)\n",
        "\n",
        "    pages = []\n",
        "    for i in range(len(doc)):\n",
        "        pix = doc[i].get_pixmap(matrix=mat, alpha=False)\n",
        "        path = f\"{PAGE_IMG_DIR}/page_{i+1}.png\"\n",
        "        pix.save(path)\n",
        "        pages.append({\"page\": i+1, \"path\": path})\n",
        "    return pages\n"
      ],
      "metadata": {
        "id": "IL9609rTpJng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "CROP_DIR = \"/content/diagram_crops\"\n",
        "os.makedirs(CROP_DIR, exist_ok=True)\n",
        "\n",
        "def extract_diagrams_from_page(page_img_path, page, source):\n",
        "    img = cv2.imread(page_img_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # --- binarize ---\n",
        "    blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
        "    thresh = cv2.adaptiveThreshold(\n",
        "        blur, 255,\n",
        "        cv2.ADAPTIVE_THRESH_MEAN_C,\n",
        "        cv2.THRESH_BINARY_INV, 15, 3\n",
        "    )\n",
        "\n",
        "    # --- STRONG closing to connect diagram parts ---\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (35, 35))\n",
        "    closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
        "\n",
        "    # --- find big connected regions ---\n",
        "    contours, _ = cv2.findContours(\n",
        "        closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "    )\n",
        "\n",
        "    crops = []\n",
        "    idx = 0\n",
        "\n",
        "    for c in contours:\n",
        "        x, y, w, h = cv2.boundingRect(c)\n",
        "\n",
        "        if w * h > 80000:   # only BIG diagram areas\n",
        "            pad = 15\n",
        "            crop = img[\n",
        "                max(0,y-pad):min(img.shape[0], y+h+pad),\n",
        "                max(0,x-pad):min(img.shape[1], x+w+pad)\n",
        "            ]\n",
        "\n",
        "            out_path = f\"{CROP_DIR}/{os.path.basename(source)}_p{page}_fig{idx}.png\"\n",
        "            cv2.imwrite(out_path, crop)\n",
        "\n",
        "            crops.append({\n",
        "                \"page\": page,\n",
        "                \"source\": source,\n",
        "                \"path\": out_path\n",
        "            })\n",
        "            idx += 1\n",
        "\n",
        "    return crops\n"
      ],
      "metadata": {
        "id": "hKiBqS9opMej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"Enter PDF path\"\n",
        "\n",
        "pages = render_pages(pdf_path)\n",
        "\n",
        "diagram_db = []\n",
        "\n",
        "for p in pages:\n",
        "    crops = extract_diagrams_from_page(p[\"path\"], p[\"page\"], pdf_path)\n",
        "    for c in crops:\n",
        "        diagram_db.append({\n",
        "            \"page\": p[\"page\"],\n",
        "            \"path\": c\n",
        "        })\n",
        "\n",
        "print(\"Total diagrams:\", len(diagram_db))\n"
      ],
      "metadata": {
        "id": "4e1XtwdYpPIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# IMPORTS\n",
        "# ============================================================\n",
        "import fitz\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import torch\n",
        "import camelot\n",
        "import gc\n",
        "import re\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    Blip2Processor,\n",
        "    Blip2ForConditionalGeneration\n",
        ")\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "from typing import List, Tuple\n",
        "\n",
        "# ============================================================\n",
        "# CONFIG\n",
        "# ============================================================\n",
        "PDF_PATH = \"PDF Path\"\n",
        "IMAGES_FOLDER = Path(\"/content//diagram_crops\")\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "BLIP2_MODEL = \"Salesforce/blip2-opt-2.7b\"\n",
        "\n",
        "PAGE_NUM_PATTERN = re.compile(r\"_p(\\d+)_\", re.IGNORECASE)\n",
        "IMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tiff\", \".webp\"}\n",
        "MAX_IMG_SIZE = 1024\n",
        "\n",
        "# ============================================================\n",
        "# TEXT EXTRACTION WITH LANGCHAIN\n",
        "# ============================================================\n",
        "def extract_text_with_langchain(pdf_path):\n",
        "    \"\"\"Extract text from PDF and split into chunks using LangChain\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    full_text = \"\"\n",
        "    page_text_map = {}\n",
        "\n",
        "    for i in range(len(doc)):\n",
        "        txt = doc[i].get_text(\"text\").strip()\n",
        "        if txt:\n",
        "            full_text += f\"\\n\\n--- Page {i + 1} ---\\n\\n{txt}\"\n",
        "            page_text_map[i + 1] = txt\n",
        "\n",
        "    # LangChain text splitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "\n",
        "    chunks = text_splitter.split_text(full_text)\n",
        "\n",
        "    # Create LangChain documents\n",
        "    documents = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        # Extract page number from chunk\n",
        "        page_match = re.search(r\"--- Page (\\d+) ---\", chunk)\n",
        "        page_num = int(page_match.group(1)) if page_match else None\n",
        "\n",
        "        documents.append(\n",
        "            Document(\n",
        "                page_content=chunk,\n",
        "                metadata={\"source\": \"pdf\", \"chunk\": i, \"page\": page_num}\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return documents, page_text_map\n",
        "\n",
        "# ============================================================\n",
        "# TABLE EXTRACTION - FIXED FOR NA VALUES\n",
        "# ============================================================\n",
        "def extract_tables(pdf_path):\n",
        "    \"\"\"Extract tables with rich context\"\"\"\n",
        "    tables = camelot.read_pdf(\n",
        "        pdf_path,\n",
        "        pages=\"all\",\n",
        "        flavor=\"lattice\",\n",
        "        strip_text=\"\\n\"\n",
        "    )\n",
        "\n",
        "    extracted = []\n",
        "    table_documents = []\n",
        "\n",
        "    for i, table in enumerate(tables):\n",
        "        df = table.df.copy()\n",
        "\n",
        "        # CRITICAL FIX: Replace pandas NA with empty string BEFORE any operations\n",
        "        df = df.fillna('')\n",
        "\n",
        "        # Clean up DataFrame - remove completely empty rows and columns\n",
        "        df = df.replace('', np.nan).dropna(how='all').dropna(axis=1, how='all').fillna('')\n",
        "\n",
        "        # Get column names from first row\n",
        "        col_names = \" | \".join(str(c) for c in df.iloc[0].tolist() if str(c).strip())\n",
        "\n",
        "        # Infer table topic\n",
        "        content_str = df.to_string().lower()\n",
        "        topic = infer_table_topic(content_str, table.page)\n",
        "\n",
        "        # Create searchable text content\n",
        "        text_content = df.to_string(index=False)\n",
        "\n",
        "        # LangChain document for table\n",
        "        table_doc = Document(\n",
        "            page_content=(\n",
        "                f\"TABLE {i+1} (Page {table.page}) - {topic}\\n\"\n",
        "                f\"Columns: {col_names}\\n\\n\"\n",
        "                f\"{text_content}\"\n",
        "            ),\n",
        "            metadata={\n",
        "                \"source\": \"table\",\n",
        "                \"table_id\": i,\n",
        "                \"page\": table.page,\n",
        "                \"topic\": topic\n",
        "            }\n",
        "        )\n",
        "        table_documents.append(table_doc)\n",
        "\n",
        "        # Store table data\n",
        "        extracted.append({\n",
        "            \"table_id\": f\"TABLE_{i+1}\",\n",
        "            \"page\": table.page,\n",
        "            \"df\": df,\n",
        "            \"columns\": df.iloc[0].tolist(),\n",
        "            \"topic\": topic,\n",
        "            \"document\": table_doc\n",
        "        })\n",
        "\n",
        "    return extracted, table_documents\n",
        "\n",
        "def infer_table_topic(content_str, page_num):\n",
        "    \"\"\"Infer table topic from content\"\"\"\n",
        "    topics = {\n",
        "        \"Based on your PDF\"\n",
        "    }\n",
        "\n",
        "    for topic, keywords in topics.items():\n",
        "        if any(kw in content_str for kw in keywords):\n",
        "            return topic\n",
        "\n",
        "    return f\" Data (Page {page_num})\"\n",
        "\n",
        "# ============================================================\n",
        "# BLIP-2 IMAGE CAPTIONING\n",
        "# ============================================================\n",
        "def load_blip2():\n",
        "    \"\"\"Load BLIP-2 model\"\"\"\n",
        "    print(\"â³ Loading BLIP-2...\")\n",
        "    processor = Blip2Processor.from_pretrained(BLIP2_MODEL)\n",
        "    model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "        BLIP2_MODEL,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    print(\"âœ… BLIP-2 loaded\")\n",
        "    return processor, model\n",
        "\n",
        "def resize_img(img, max_side):\n",
        "    \"\"\"Resize image maintaining aspect ratio\"\"\"\n",
        "    w, h = img.size\n",
        "    if max(w, h) <= max_side:\n",
        "        return img\n",
        "    r = max_side / max(w, h)\n",
        "    return img.resize((int(w * r), int(h * r)), Image.LANCZOS)\n",
        "\n",
        "def parse_page_num(name):\n",
        "    \"\"\"Extract page number from filename\"\"\"\n",
        "    m = PAGE_NUM_PATTERN.search(name)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "def get_page_context(page_num, page_text_map, window=0):\n",
        "    \"\"\"Get text context from page\"\"\"\n",
        "    if not page_num or page_num not in page_text_map:\n",
        "        return \"\"\n",
        "\n",
        "    text = page_text_map[page_num]\n",
        "    if len(text) > 600:\n",
        "        text = text[:600] + \"...\"\n",
        "\n",
        "    return f\"Page {page_num} context: {text}\"\n",
        "\n",
        "def infer_diagram_content(filename, page_num):\n",
        "    \"\"\"Infer diagram type from filename\"\"\"\n",
        "    filename_lower = filename.lower()\n",
        "\n",
        "    keywords_map = {\n",
        "        'p77': \"Discription of your own\",\n",
        "    }\n",
        "\n",
        "    for keyword, description in keywords_map.items():\n",
        "        if keyword in filename_lower:\n",
        "            return f\"{description} (Page {page_num})\"\n",
        "\n",
        "    return f\"Technical diagram from  electrical installation (Page {page_num})\"\n",
        "\n",
        "def caption_all_images(folder, processor, model, page_text_map):\n",
        "    \"\"\"Caption all images and create LangChain documents\"\"\"\n",
        "    prompt = \"Question: What does this electrical diagram show? Answer:\"\n",
        "    device = next(model.parameters()).device\n",
        "    image_documents = []\n",
        "    image_data = []\n",
        "\n",
        "    paths = sorted(p for p in folder.iterdir() if p.suffix.lower() in IMG_EXTS)\n",
        "    print(f\"ğŸ“¸ Captioning {len(paths)} images...\")\n",
        "\n",
        "    for idx, p in enumerate(paths, 1):\n",
        "        try:\n",
        "            page = parse_page_num(p.name)\n",
        "            img = resize_img(Image.open(p).convert(\"RGB\"), MAX_IMG_SIZE)\n",
        "\n",
        "            inputs = processor(img, prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                out = model.generate(**inputs, max_new_tokens=80)\n",
        "\n",
        "            caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "            # Get context\n",
        "            page_ctx = get_page_context(page, page_text_map, window=0)\n",
        "            diagram_info = infer_diagram_content(p.name, page)\n",
        "\n",
        "            # Create LangChain document\n",
        "            image_doc = Document(\n",
        "                page_content=(\n",
        "                    f\"Image: {p.name}\\n\"\n",
        "                    f\"Page: {page}\\n\"\n",
        "                    f\"Type: {diagram_info}\\n\"\n",
        "                    f\"Visual description: {caption}\\n\"\n",
        "                    f\"{page_ctx}\"\n",
        "                ),\n",
        "                metadata={\n",
        "                    \"source\": \"image\",\n",
        "                    \"filename\": p.name,\n",
        "                    \"page\": page,\n",
        "                    \"path\": str(p)\n",
        "                }\n",
        "            )\n",
        "\n",
        "            image_documents.append(image_doc)\n",
        "            image_data.append({\n",
        "                \"document\": image_doc,\n",
        "                \"path\": p,\n",
        "                \"page\": page,\n",
        "                \"filename\": p.name,\n",
        "                \"caption\": caption\n",
        "            })\n",
        "\n",
        "            if idx % 10 == 0:\n",
        "                print(f\"  Processed {idx}/{len(paths)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error processing {p.name}: {e}\")\n",
        "\n",
        "    print(f\"âœ… Captioned {len(image_data)} images\")\n",
        "    return image_data, image_documents\n",
        "\n",
        "# ============================================================\n",
        "# BUILD VECTOR STORE WITH LANGCHAIN\n",
        "# ============================================================\n",
        "def build_vectorstore(text_docs, table_docs, image_docs):\n",
        "    \"\"\"Build Chroma vector store with all documents\"\"\"\n",
        "    print(\"ğŸ” Building vector store with LangChain...\")\n",
        "\n",
        "    # Combine all documents\n",
        "    all_documents = text_docs + table_docs + image_docs\n",
        "\n",
        "    print(f\"   Total documents: {len(all_documents)}\")\n",
        "    print(f\"   - Text chunks: {len(text_docs)}\")\n",
        "    print(f\"   - Tables: {len(table_docs)}\")\n",
        "    print(f\"   - Images: {len(image_docs)}\")\n",
        "\n",
        "    # Create embeddings\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        model_kwargs={'device': 'cpu'}\n",
        "    )\n",
        "\n",
        "    # Create Chroma vector store\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=all_documents,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=\"./chroma_db\"\n",
        "    )\n",
        "\n",
        "    print(\"âœ… Vector store built\")\n",
        "    return vectorstore\n",
        "\n",
        "# ============================================================\n",
        "# LOAD LLM\n",
        "# ============================================================\n",
        "def load_llm():\n",
        "    \"\"\"Load Mistral-7B with 4-bit quantization\"\"\"\n",
        "    bnb = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    print(\"â³ Loading Mistral-7B in 4-bit...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    print(\"âœ… Mistral-7B loaded\")\n",
        "    print(f\"   GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# ============================================================\n",
        "# INITIALIZATION SEQUENCE\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# Step 1: Extract text and tables\n",
        "print(\"\\nğŸ“„ Step 1: Extracting text and tables...\")\n",
        "text_documents, page_text_map = extract_text_with_langchain(PDF_PATH)\n",
        "tables_data, table_documents = extract_tables(PDF_PATH)\n",
        "print(f\"   âœ“ {len(text_documents)} text chunks\")\n",
        "print(f\"   âœ“ {len(tables_data)} tables\")\n",
        "\n",
        "# Step 2: BLIP-2 image captioning\n",
        "print(\"\\nğŸ–¼ï¸ Step 2: Captioning images with BLIP-2...\")\n",
        "blip_proc, blip_model = load_blip2()\n",
        "images_data, image_documents = caption_all_images(\n",
        "    IMAGES_FOLDER, blip_proc, blip_model, page_text_map\n",
        ")\n",
        "\n",
        "# Step 3: Free BLIP-2\n",
        "print(\"\\nğŸ—‘ï¸ Step 3: Freeing BLIP-2...\")\n",
        "del blip_proc, blip_model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(f\"   GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
        "\n",
        "# Step 4: Build vector store\n",
        "print(\"\\nğŸ” Step 4: Building vector store...\")\n",
        "vectorstore = build_vectorstore(text_documents, table_documents, image_documents)\n",
        "\n",
        "# Step 5: Load LLM\n",
        "print(\"\\nğŸ¤– Step 5: Loading Mistral-7B...\")\n",
        "llm_model, tokenizer = load_llm()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"âœ… ALL SYSTEMS READY!\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# CHATBOT FUNCTION - FIXED FOR NA VALUES\n",
        "# ============================================================\n",
        "def chatbot(query, history=None):\n",
        "    \"\"\"\n",
        "    Main chatbot function using LangChain retrieval\n",
        "    Returns: (markdown_text, list_of_images)\n",
        "    \"\"\"\n",
        "    if not query.strip():\n",
        "        return \"Please enter a question.\", []\n",
        "\n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    # RETRIEVE DOCUMENTS WITH LANGCHAIN\n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "    # Retrieve top k documents\n",
        "    retrieved_docs = vectorstore.similarity_search(query, k=12)\n",
        "\n",
        "    # Separate by source type\n",
        "    text_docs = []\n",
        "    table_docs = []\n",
        "    image_docs = []\n",
        "\n",
        "    for doc in retrieved_docs:\n",
        "        source = doc.metadata.get(\"source\")\n",
        "        if source == \"pdf\":\n",
        "            text_docs.append(doc)\n",
        "        elif source == \"table\":\n",
        "            table_docs.append(doc)\n",
        "        elif source == \"image\":\n",
        "            image_docs.append(doc)\n",
        "\n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    # BUILD CONTEXT FROM RETRIEVED DOCUMENTS\n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "    # Text context - take top 4 chunks\n",
        "    text_ctx_parts = []\n",
        "    for doc in text_docs[:4]:\n",
        "        content = doc.page_content\n",
        "        # Remove page markers for cleaner context\n",
        "        content = re.sub(r'--- Page \\d+ ---', '', content).strip()\n",
        "        if len(content) > 700:\n",
        "            content = content[:700] + \"...\"\n",
        "        text_ctx_parts.append(content)\n",
        "\n",
        "    text_ctx = \"\\n\\n\".join(text_ctx_parts)\n",
        "\n",
        "    # Table context - find the actual table data\n",
        "    table_md = \"\"\n",
        "    table_ctx = \"\"\n",
        "    displayed_tables = []\n",
        "\n",
        "    for doc in table_docs[:3]:  # Top 3 tables\n",
        "        table_id = doc.metadata.get(\"table_id\")\n",
        "\n",
        "        # Find the corresponding table data\n",
        "        for t_data in tables_data:\n",
        "            if t_data[\"table_id\"] == f\"TABLE_{table_id+1}\":\n",
        "                df_display = t_data[\"df\"].copy()\n",
        "\n",
        "                # CRITICAL FIX: fillna before any operations\n",
        "                df_display = df_display.fillna('')\n",
        "\n",
        "                # Use first row as headers\n",
        "                df_display.columns = df_display.iloc[0]\n",
        "                df_display = df_display[1:].reset_index(drop=True)\n",
        "\n",
        "                # Ensure no NA values remain\n",
        "                df_display = df_display.fillna('')\n",
        "\n",
        "                # Markdown for display - now safe from NA errors\n",
        "                try:\n",
        "                    table_md += (\n",
        "                        f\"\\n### ğŸ“Š {t_data['table_id']} - Page {t_data['page']} - {t_data['topic']}\\n\\n\"\n",
        "                        + df_display.to_markdown(index=False)\n",
        "                        + \"\\n\"\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    # Fallback to HTML table if markdown fails\n",
        "                    table_md += (\n",
        "                        f\"\\n### ğŸ“Š {t_data['table_id']} - Page {t_data['page']} - {t_data['topic']}\\n\\n\"\n",
        "                        + df_display.to_html(index=False)\n",
        "                        + \"\\n\"\n",
        "                    )\n",
        "\n",
        "                # Context for LLM (first 10 rows only to save tokens)\n",
        "                table_ctx += (\n",
        "                    f\"[Table {t_data['table_id']} on page {t_data['page']} - {t_data['topic']}]\\n\"\n",
        "                    + df_display.head(10).to_string(index=False)\n",
        "                    + \"\\n\\n\"\n",
        "                )\n",
        "\n",
        "                displayed_tables.append(t_data['table_id'])\n",
        "                break\n",
        "\n",
        "    # Image context\n",
        "    relevant_images = []\n",
        "    img_ctx = \"\"\n",
        "\n",
        "    for doc in image_docs[:5]:  # Top 5 images\n",
        "        filename = doc.metadata.get(\"filename\")\n",
        "        page = doc.metadata.get(\"page\")\n",
        "        img_path = doc.metadata.get(\"path\")\n",
        "\n",
        "        # Find the actual image data\n",
        "        for img_data in images_data:\n",
        "            if img_data[\"filename\"] == filename:\n",
        "                # For Gallery - return PIL Image object\n",
        "                try:\n",
        "                    pil_img = Image.open(img_path)\n",
        "                    label = f\"ğŸ“„ Page {page} | {filename}\"\n",
        "                    relevant_images.append((pil_img, label))\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading image {img_path}: {e}\")\n",
        "\n",
        "                # For LLM context\n",
        "                img_ctx += (\n",
        "                    f\"[Image: {filename} from Page {page}]\\n\"\n",
        "                    f\"Description: {img_data['caption']}\\n\\n\"\n",
        "                )\n",
        "                break\n",
        "\n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    # BUILD LLM PROMPT\n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "    context_parts = []\n",
        "\n",
        "    if text_ctx:\n",
        "        context_parts.append(f\"ğŸ“„ DOCUMENT TEXT:\\n{text_ctx}\")\n",
        "\n",
        "    if table_ctx:\n",
        "        context_parts.append(f\"ğŸ“Š RELEVANT TABLES:\\n{table_ctx}\")\n",
        "\n",
        "    if img_ctx:\n",
        "        context_parts.append(f\"ğŸ–¼ï¸ RELEVANT DIAGRAMS:\\n{img_ctx}\")\n",
        "\n",
        "    if not context_parts:\n",
        "        return \"âŒ No relevant information found in the document.\", []\n",
        "\n",
        "    combined_context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST] You are a specialized technical assistant for the manual.\n",
        "\n",
        "Use the provided context to answer the user's question accurately and technically.\n",
        "\n",
        "CONTEXT:\n",
        "{combined_context}\n",
        "\n",
        "USER QUESTION: {query}\n",
        "\n",
        "\n",
        "\n",
        "[/INST]\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    # GENERATE ANSWER\n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "    # Truncate if needed\n",
        "    tokens = tokenizer.encode(prompt, truncation=False)\n",
        "    if len(tokens) > 2900:\n",
        "        tokens = tokens[:2900]\n",
        "        prompt = tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "        if \"[/INST]\" not in prompt[-100:]:\n",
        "            prompt += \"\\n[/INST]\\n\\nAnswer:\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=3000\n",
        "    ).to(llm_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = llm_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "            do_sample=True,\n",
        "            repetition_penalty=1.15,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract answer\n",
        "    if \"[/INST]\" in full_response:\n",
        "        answer = full_response.split(\"[/INST]\")[-1].strip()\n",
        "    else:\n",
        "        answer = full_response.strip()\n",
        "\n",
        "    if answer.startswith(\"Answer:\"):\n",
        "        answer = answer[7:].strip()\n",
        "\n",
        "    # Clean up any context repetition\n",
        "    # if len(answer) > 1500:\n",
        "    #     answer = answer[:1500] + \"...\"\n",
        "\n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "    # FORMAT OUTPUT\n",
        "    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "    output_md = f\"### ğŸ¤– Answer\\n\\n{answer}\\n\"\n",
        "\n",
        "    # Add source pages\n",
        "    source_pages = set()\n",
        "    for doc in text_docs[:4]:\n",
        "        if doc.metadata.get(\"page\"):\n",
        "            source_pages.add(doc.metadata[\"page\"])\n",
        "\n",
        "    if source_pages:\n",
        "        pages_str = \", \".join([f\"Page {p}\" for p in sorted(source_pages)])\n",
        "        output_md += f\"\\n*ğŸ“„ Sources: {pages_str}*\\n\"\n",
        "\n",
        "    # Add tables\n",
        "    if table_md:\n",
        "        output_md += f\"\\n---\\n{table_md}\"\n",
        "        if displayed_tables:\n",
        "            output_md += f\"\\n*ğŸ“Š Displayed: {', '.join(displayed_tables)}*\\n\"\n",
        "\n",
        "    # Note about images\n",
        "    if relevant_images:\n",
        "        output_md += f\"\\n---\\n\\nğŸ“ **{len(relevant_images)} relevant diagram(s) shown below**\\n\"\n",
        "\n",
        "    return output_md, relevant_images\n",
        "\n",
        "# ============================================================\n",
        "# GRADIO INTERFACE\n",
        "# ============================================================\n",
        "\n",
        "custom_css = \"\"\"\n",
        "#chatbot {\n",
        "    height: 550px;\n",
        "}\n",
        "#gallery {\n",
        "    min-height: 450px;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(\n",
        "    title=\"Technical Assistant\",\n",
        "    css=custom_css,\n",
        "    theme=gr.themes.Soft()\n",
        ") as demo:\n",
        "\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # Chatbot\n",
        "\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            chatbot_display = gr.Chatbot(\n",
        "                label=\"Conversation\",\n",
        "                elem_id=\"chatbot\",\n",
        "                height=550,\n",
        "                avatar_images=(None, \"ğŸ¤–\")\n",
        "            )\n",
        "\n",
        "            with gr.Row():\n",
        "                msg_input = gr.Textbox(\n",
        "                    label=\"Your Question\",\n",
        "                    placeholder=\"Example: What does Table 1 show about generator routes?\",\n",
        "                    lines=3,\n",
        "                    scale=4\n",
        "                )\n",
        "\n",
        "            with gr.Row():\n",
        "                submit_btn = gr.Button(\"ğŸš€ Send\", variant=\"primary\", scale=2)\n",
        "                clear_btn = gr.Button(\"ğŸ—‘ï¸ Clear\", scale=1)\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### ğŸ“Š Technical Diagrams\")\n",
        "\n",
        "            image_gallery = gr.Gallery(\n",
        "                label=\"Relevant Diagrams\",\n",
        "                elem_id=\"gallery\",\n",
        "                columns=2,\n",
        "                rows=2,\n",
        "                height=450,\n",
        "                object_fit=\"contain\",\n",
        "                show_label=False\n",
        "            )\n",
        "\n",
        "\n",
        "    # Event handlers\n",
        "    def respond(message, chat_history):\n",
        "        if not message.strip():\n",
        "            return chat_history, []\n",
        "\n",
        "        bot_response, images = chatbot(message)\n",
        "        chat_history.append((message, bot_response))\n",
        "\n",
        "        return chat_history, images\n",
        "\n",
        "    def clear_all():\n",
        "        return [], []\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=respond,\n",
        "        inputs=[msg_input, chatbot_display],\n",
        "        outputs=[chatbot_display, image_gallery]\n",
        "    ).then(\n",
        "        lambda: \"\",\n",
        "        outputs=[msg_input]\n",
        "    )\n",
        "\n",
        "    msg_input.submit(\n",
        "        fn=respond,\n",
        "        inputs=[msg_input, chatbot_display],\n",
        "        outputs=[chatbot_display, image_gallery]\n",
        "    ).then(\n",
        "        lambda: \"\",\n",
        "        outputs=[msg_input]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=clear_all,\n",
        "        outputs=[chatbot_display, image_gallery]\n",
        "    )\n",
        "\n",
        "print(\"\\nğŸš€ Launching Gradio interface...\")\n",
        "demo.launch(share=True, server_port=7860, debug=True)"
      ],
      "metadata": {
        "id": "tJRDIeL_qMuA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}